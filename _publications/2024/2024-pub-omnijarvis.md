---
title:          "OmniJARVIS: Unified Vision-Language-Action Tokenization Enables Open-World Instruction Following Agents"
date:           2024-07-10 00:01:00 +0800
selected:       false
pub:            "Neural Information Processing Systems (NeurIPS'24)"
# pub_pre:        "Submitted to "
# pub_post:       'Under review.'
# pub_last:       ' <span class="badge/ badge-pill badge-publication badge-success">Spotlight</span>'
pub_date:       "2024"

abstract: >-
  This paper presents OmniJARVIS, a novel Vision-Language-Action (VLA) model for open-world instructionfollowing agents in Minecraft. Compared to prior works that either emit textual goals to separate controllers or produce the control command directly, OmniJARVIS seeks a different path to ensure both strong reasoning and efficient decision-making capabilities via unified tokenization of multimodal interaction data. 
cover: https://raw.githubusercontent.com/phython96/Images/master/omnijarvis-logo.png
authors:
  - Zihao Wang
  - Shaofei Cai
  - Zhancun Mu
  - Haowei Lin
  - Ceyao Zhang
  - Xuejie Liu
  - Qing Li
  - Anji Liu
  - Xiaojian Ma
  - Yitao Liang
links:
  Paper: https://arxiv.org/pdf/2407.00114
  Page: https://omnijarvis.github.io/
  Code: https://github.com/CraftJarvis/OmniJarvis
  Twittr: https://x.com/jeasinema/status/1808346701205516395
---
