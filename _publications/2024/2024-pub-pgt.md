---
title:          "Optimizing Latent Goal by Learning from Trajectory Preference"
date:           2024-09-10 00:01:00 +0800
selected:       false
pub:            "arxiv"
# pub_pre:        "Submitted to "
# pub_post:       'Under review.'
# pub_last:       ' <span class="badge/ badge-pill badge-publication badge-success">Spotlight</span>'
pub_date:       "2024"

abstract: >-
  This paper proposes a framework named Preference Goal Tuning (PGT). PGT allows an instruction following policy to interact with the environment to collect several trajectories, which will be categorized into positive and negative samples based on preference. We use preference learning to fine-tune the initial goal latent representation with the categorized trajectories while keeping the policy backbone frozen. 
cover: https://raw.githubusercontent.com/phython96/Images/master/pgt-logo.png
authors:
  - Guangyu Zhao*  
  - Kewei Lian*
  - Haowei Lin
  - Haobo Fu
  - Qiang Fu
  - Shaofei Cai
  - Zihao Wang
  - Yitao Liang
links:
  Paper: https://arxiv.org/abs/2412.02125
---
