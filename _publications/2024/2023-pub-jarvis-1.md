---
title:          "JARVIS-1: Open-World Multi-task Agents with Memory-Augmented Multimodal Language Models"
date:           2024-06-01 00:01:00 +0800
selected:       false 
pub:            "IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI'24)"
# pub_pre:        "Submitted to "
# pub_post:       'Under review.'
# pub_last:       ' <span class="badge badge-pill badge-publication badge-success">Spotlight</span>'
pub_date:       "2024"

abstract: >-
  This paper introduces JARVIS-1, an open-world agent that can perceive multimodal input (visual observations and human instructions), generate sophisticated plans, and perform embodied control, all within the popular yet challenging open-world Minecraft universe. Specifically, we develop JARVIS-1 on top of pre-trained multimodal language models, which map visual observations and textual instructions to plans. The plans will be ultimately dispatched to the goal-conditioned controllers. 
cover: https://raw.githubusercontent.com/phython96/Images/master/jarvis-1-logo.png 
authors:
  - Zihao Wang
  - Shaofei Cai 
  - Anji Liu
  - Yonggang Jin
  - Jinbing Hou
  - Bowei Zhang
  - Haowei Lin
  - Zhaofeng He
  - Zilong Zheng
  - Yaodong Yang
  - Xiaojian Ma
  - Yitao Liang
links:
  Paper: https://arxiv.org/pdf/2311.05997
  Code: https://github.com/CraftJarvis/JARVIS-1
  Page: https://craftjarvis.github.io/JARVIS-1/
  Twitter: https://x.com/jeasinema/status/1723900032653643796
  Media: https://mp.weixin.qq.com/s/4SyX4QCdu9rBptRvOQIwXg
---
